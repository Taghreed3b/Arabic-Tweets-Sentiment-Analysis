# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ex2dMStpSMc2aBuXj6vW08dN2rQxrDHb

#Prepared By: Taghreed Alghamdi , Salma Alzahrani
#EMIS 602: Big Data Technologies
#Group Project â€“ Twitter Sentiment Analysis
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF,StringIndexer
from pyspark.ml import Pipeline

spark = SparkSession.builder.getOrCreate()

train = spark.read.csv('/content/drive/MyDrive/train_arabic_tweets.tsv' , sep='\t', encoding='utf-8', header=False).toDF('target', 'text')
test = spark.read.csv('/content/drive/MyDrive/test_data.tsv' , sep='\t', encoding='utf-8', header=False).toDF('target', 'text')

print("Dataset schema:")
train.printSchema()
print("Training dataset:")
train.show(50)

import pandas as pd
print(train.toPandas().groupby(['target']).size())

(train_set, val_set) = train.randomSplit((0.9,0.1), seed=42)

ar_stop_list = open("/content/drive/MyDrive/arabic_stopwords.txt", encoding="utf-8")
stop_words = ar_stop_list.read().split('\n')

tokenizer = Tokenizer(inputCol='text',outputCol='tokens')
stopwords_remover = StopWordsRemover(inputCol='tokens',outputCol='filtered_tokens').setStopWords(stop_words)
vectorizer = CountVectorizer(inputCol='filtered_tokens',outputCol='raw_features')
idf = IDF(inputCol='raw_features',outputCol='vectorized_features')
label_stringIdx = StringIndexer(inputCol='target',outputCol='label')
pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf,label_stringIdx])

pipeline_model = pipeline.fit(train_set)

train_df = pipeline_model.transform(train_set)
val_df = pipeline_model.transform(val_set)

print("\nNew columns in training dataset:")
print(train_df.columns)
print("\nTransformed training dataset:")
train_df.show(50)

lr = LogisticRegression(maxIter=100, featuresCol='vectorized_features', labelCol='label')
lr_model = lr.fit(train_df)

val_predictions = lr_model.transform(val_df)

print("\nNew columns in validation dataset:")
print(val_predictions.columns)
print("\nTransformed validation dataset:")

val_predictions.select('target','label','prediction','text').show()

val_vaccuracy = val_predictions.filter(val_predictions.label == val_predictions.prediction).count() / float(val_set.count())
print("Validation Accuracy Score: {0:.4f}".format(val_vaccuracy))

print("\n\nPredictions on testing dataset:\n")
print("Dataset schema:")
test.printSchema()
print("Test dataset:")
test.show(20)

import pandas as pd
print(test.toPandas().groupby(['target']).size())

(train_set, val_set) = test.randomSplit((0.9,0.1), seed=42)

ar_stop_list = open("/content/drive/MyDrive/arabic_stopwords.txt", encoding="utf-8")
stop_words = ar_stop_list.read().split('\n')

tokenizer = Tokenizer(inputCol='text',outputCol='tokens')
stopwords_remover = StopWordsRemover(inputCol='tokens',outputCol='filtered_tokens').setStopWords(stop_words)
vectorizer = CountVectorizer(inputCol='filtered_tokens',outputCol='raw_features')
idf = IDF(inputCol='raw_features',outputCol='vectorized_features')
label_stringIdx = StringIndexer(inputCol='target',outputCol='label')
pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf,label_stringIdx])

pipeline_model = pipeline.fit(train_set)

train_df = pipeline_model.transform(train_set)
val_df = pipeline_model.transform(val_set)

print("\nNew columns in training dataset:")
print(train_df.columns)
print("\nTransformed training dataset:")
train_df.show(20)

lr = LogisticRegression(maxIter=100, featuresCol='vectorized_features', labelCol='label')
lr_model = lr.fit(train_df)

val_predictions = lr_model.transform(val_df)

print("\nNew columns in validation dataset:")
print(val_predictions.columns)
print("\nTransformed validation dataset:")

val_predictions.show()

val_predictions.select('target','label','prediction','text').show()

val_vaccuracy = val_predictions.filter(val_predictions.label == val_predictions.prediction).count() / float(val_set.count())
print("Validation Accuracy Score: {0:.4f}".format(val_vaccuracy))